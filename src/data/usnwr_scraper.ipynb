{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved to hospital_rankings.csv\n",
      "Saved to hospital_rankings_pivoted.csv\n"
     ]
    }
   ],
   "source": [
    "from curl_cffi.requests import Session # Make sure you've pip installed curl-cffi\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import html \n",
    "import os\n",
    "\n",
    "DOWNLOAD_DIR = \"/Users/jiturner/Downloads\" # Replace with the local directory where you want to save the CSVs\n",
    "\n",
    "specialties = [\n",
    "    \"cancer\", \"cardiology-and-heart-surgery\", \"diabetes-and-endocrinology\",\n",
    "    \"ear-nose-and-throat\", \"gastroenterology-and-gi-surgery\", \"geriatric-care\",\n",
    "    \"kidney-failure\",\n",
    "    \"gynecology\", \"neurology-and-neurosurgery\", \"ophthalmology\", \"orthopedics\",\n",
    "    \"psychiatry\", \"pulmonology\", \"rehabilitation\", \"rheumatology\", \"urology\"\n",
    "]\n",
    "\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.G' ' (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:126.0) Gecko/20100101 Firefox/126.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.5 Safari/605.1.15',\n",
    "]\n",
    "\n",
    "all_rankings = []\n",
    "base_url = \"https://health.usnews.com\"\n",
    "REQUEST_TIMEOUT = 20\n",
    "\n",
    "json_pattern = re.compile(r\"window\\['__PAGE_CONTEXT_QUERY_STATE__'\\] = (\\{.*?\\});\", re.DOTALL)\n",
    "\n",
    "session = Session()\n",
    "session.timeout = REQUEST_TIMEOUT\n",
    "\n",
    "for specialty in specialties:\n",
    "    print(f\"--- Scraping {specialty} ---\")\n",
    "    page_url = f\"{base_url}/best-hospitals/rankings/{specialty}\"\n",
    "    \n",
    "    session.headers['User-Agent'] = random.choice(USER_AGENTS)\n",
    "    time.sleep(random.uniform(2.0, 5.0))\n",
    "    \n",
    "    try:\n",
    "        print(f\"Requesting {page_url}...\")\n",
    "        response = session.get(page_url, impersonate=\"chrome110\")\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        match = json_pattern.search(response.text)\n",
    "        if not match:\n",
    "            print(f\"Could not find JSON data for {specialty}\")\n",
    "            continue\n",
    "            \n",
    "        json_text = match.group(1)\n",
    "        \n",
    "        # --- START: ROBUST PARSING LOGIC ---\n",
    "        \n",
    "        # 1. Un-escape HTML entities like &amp;\n",
    "        json_text = html.unescape(json_text)\n",
    "        \n",
    "        # 2. Replace undefined\n",
    "        json_text = json_text.replace(':undefined', ':null')\n",
    "        json_text = json_text.replace(',undefined', ',null')\n",
    "        json_text = json_text.replace('[undefined', '[null')\n",
    "\n",
    "        # 3. Aggressive removal of link-data attributes\n",
    "        json_text = re.sub(r'\\s+link-data=\\\\\"[^>]*?\\\\\"(?=>)', '', json_text)\n",
    "        \n",
    "        # 4. Fallback removal for link-data\n",
    "        if 'link-data=' in json_text:\n",
    "            json_text = re.sub(r'\\s+link-data=\\\\\"[^\\u003E]*?\\\\\"', '', json_text)\n",
    "        \n",
    "        # 5. NEW: Remove style attributes (which have unescaped quotes)\n",
    "        json_text = re.sub(r'\\s+style=\\\\\"[^\"]*?\\\\\"', '', json_text)\n",
    "\n",
    "        # 6. NEW: Fallback removal for style\n",
    "        if 'style=\\\\\"' in json_text:\n",
    "            json_text = re.sub(r'\\s+style=\\\\\"[^\\u003E]*?\\\\\"', '', json_text)\n",
    "        \n",
    "        # --- END: ROBUST PARSING LOGIC ---\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(json_text)\n",
    "        except json.JSONDecodeError as e:\n",
    "            # If it still fails, we'll save the file and move on\n",
    "            print(f\"!!! JSON PARSE FAILED for {specialty}: {e}\")\n",
    "            filename = f\"failed_json_FINAL_{specialty}.txt\"\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(json_text)\n",
    "            print(f\"Saved final failed text to {filename}\")\n",
    "            \n",
    "            # Print debug context\n",
    "            error_pos = e.pos\n",
    "            start = max(0, error_pos - 150)\n",
    "            end = min(len(json_text), error_pos + 150)\n",
    "            print(f\"\\nContext around position {error_pos}:\")\n",
    "            print(json_text[start:end])\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        # 3. Find the key data\n",
    "        page_data_key = 'src/containers/pages/health/hospitals/search/index.js'\n",
    "        page_data = data.get(page_data_key, {}).get('data', {})\n",
    "        \n",
    "        # 4. Get initial matches and pagination info\n",
    "        matches = page_data.get('matches', [])\n",
    "        has_next_page = page_data.get('hasNextPage', False)\n",
    "        \n",
    "        if not matches:\n",
    "            print(f\"No matches found for {specialty}\")\n",
    "            continue\n",
    "            \n",
    "        specialty_id = matches[0]['ranking']['specialty_id']\n",
    "        print(f\"Found {len(matches)} initial hospitals (Page 1). (hasNextPage: {has_next_page})\")\n",
    "\n",
    "        for hospital in matches:\n",
    "            all_rankings.append({\n",
    "                'specialty': hospital['ranking']['specialty_name'],\n",
    "                'rank': hospital['ranking']['rank'],\n",
    "                'name': hospital['name'],\n",
    "                'url': f\"{base_url}{hospital['url']}\"\n",
    "            })\n",
    "\n",
    "        # 5. --- Handle Pagination (Hardcoded to 5 pages total) ---\n",
    "        page = 2\n",
    "        \n",
    "        while has_next_page and page <= 5: \n",
    "            print(f\"Fetching page {page} for {specialty}...\")\n",
    "            time.sleep(random.uniform(1.0, 3.0))\n",
    "            \n",
    "            api_url = f\"{base_url}/best-hospitals/search-data\"\n",
    "            \n",
    "            params = {\n",
    "                'page': page,\n",
    "                'specialty_id': specialty_id,\n",
    "                'type': 'adult',\n",
    "                'sort': 'ranking'\n",
    "            }\n",
    "            \n",
    "            api_res = session.get(api_url, params=params, impersonate=\"chrome110\")\n",
    "            api_res.raise_for_status() # Add this to catch bad API requests\n",
    "            \n",
    "            # --- THIS IS THE FIX ---\n",
    "            # The API response IS the data. There is no 'data' key.\n",
    "            api_data = api_res.json()\n",
    "            # --- END FIX ---\n",
    "            \n",
    "            page_matches = api_data.get('matches', [])\n",
    "            has_next_page = api_data.get('hasNextPage', False)\n",
    "            \n",
    "            for hospital in page_matches:\n",
    "                all_rankings.append({\n",
    "                    'specialty': hospital['ranking']['specialty_name'],\n",
    "                    'rank': hospital['ranking']['rank'],\n",
    "                    'name': hospital['name'],\n",
    "                    'url': f\"{base_url}{hospital['url']}\"\n",
    "                })\n",
    "            \n",
    "            page += 1\n",
    "            \n",
    "        if has_next_page and page > 5:\n",
    "            print(f\"Reached page limit (5). Stopping pagination for {specialty}.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {specialty}: {e}\")\n",
    "        print(\"Moving to next specialty...\")\n",
    "\n",
    "# --- All done, show the results ---\n",
    "df = pd.DataFrame(all_rankings)\n",
    "print(\"\\n\\n--- SCRAPING COMPLETE ---\")\n",
    "\n",
    "if not df.empty:\n",
    "\n",
    "    # Now save a pivoted version with cumulative columns\n",
    "    df['Institution'] = df['url'].str.replace('https://health.usnews.com/best-hospitals/area/', '')\n",
    "    df['Institution'] = df['url'].apply(lambda x:\n",
    "                                    x.replace('https://health.usnews.com/best-hospitals/area/', '').split('/')[1].replace('-', ' ')\n",
    "                                    )\n",
    "    df['Institution'] = df['Institution'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))\n",
    "    df['Institution'] = df['Institution'].str.strip()\n",
    "    df['Institution'] = df['Institution'].str.title()\n",
    "\n",
    "    # Make \"Institution\" be the first column\n",
    "    cols = df.columns.tolist()\n",
    "    cols = ['Institution'] + [col for col in cols if col != 'Institution']\n",
    "    df = df[cols]\n",
    "\n",
    "    df.to_csv(os.path.join(DOWNLOAD_DIR, \"raw_hospital_rankings.csv\"), index=False)\n",
    "    print(\"\\nSaved to hospital_rankings.csv\")\n",
    "\n",
    "\n",
    "    pivot_df = df.pivot_table(\n",
    "        index='Institution',\n",
    "        columns='specialty',\n",
    "        values='rank'\n",
    "    )\n",
    "\n",
    "    pivot_df_with_cumulative_columns = pivot_df.copy()\n",
    "\n",
    "    pivot_df_with_cumulative_columns['#n_ranked_specialties'] = pivot_df.count(axis=1)\n",
    "\n",
    "    # 2. Count of specialties ranked in the Top 10 (rank < 11)\n",
    "    pivot_df_with_cumulative_columns['#n_top10_specialties'] = (pivot_df <= 10).sum(axis=1)\n",
    "\n",
    "    # 3. Count of specialties ranked #1\n",
    "    pivot_df_with_cumulative_columns['#n_top1_specialties'] = (pivot_df == 1).sum(axis=1)\n",
    "\n",
    "    pivot_df_with_cumulative_columns = pivot_df_with_cumulative_columns.sort_values(\n",
    "        by='#n_ranked_specialties',\n",
    "        ascending=False\n",
    "    )\n",
    "\n",
    "    pivot_df_with_cumulative_columns = pivot_df_with_cumulative_columns.reset_index().rename(columns={'Institution': 'Institution'})\n",
    "\n",
    "    pivot_df_with_cumulative_columns.columns.name = None\n",
    "\n",
    "    pivot_df_with_cumulative_columns.to_csv(os.path.join(DOWNLOAD_DIR, \"hospital_rankings_pivoted.csv\"), index=False)\n",
    "    print(\"Saved to hospital_rankings_pivoted.csv\")\n",
    "\n",
    "else:\n",
    "    print(\"No data was scraped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
